# Environment Variables
# Copy this file to .env and fill in your values

# =============================================================================
# API Keys (set the one you need based on your backend choice)
# =============================================================================

# OpenAI API Key (for --backend openai, default)
OPENAI_API_KEY=your_openai_api_key_here

# OpenRouter API Key (for --backend openrouter)
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# =============================================================================
# LM Studio (Local Inference)
# =============================================================================
# No API key needed for LM Studio!
# Just start LM Studio, load a vision-capable model, and use:
#   medical-sorter /path/to/docs --backend lmstudio --model "your-model-name"
#
# Default LM Studio URL: http://localhost:1234/v1
# Override with --base-url if using a different port

# =============================================================================
# Model Recommendations
# =============================================================================
# OpenAI:      gpt-4o (default), gpt-4o-mini
# OpenRouter:  openai/gpt-4o, anthropic/claude-3.5-sonnet, google/gemini-pro-vision
# LM Studio:   Use any vision-capable model (e.g., llava, bakllava)
